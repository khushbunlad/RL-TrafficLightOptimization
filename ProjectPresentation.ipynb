{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b5af96",
   "metadata": {},
   "source": [
    "# Reinforcement Learning-Based Traffic Light Optimization\n",
    "\n",
    "### Group 5\n",
    "| #     | Name |\n",
    "|-------|---------------|\n",
    "|8965985|Pradeepti Kasam|\n",
    "|9027375|Khushbu Lad|\n",
    "|8944328|Akshata Madhav|\n",
    "|8914803|Rohit Totlani| \n",
    "|8964515|Neha Yadav|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de426b90",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This project aims to develop a reinforcement learning (RL) agent capable of optimizing traffic flow by managing a single traffic light at an intersection. The agent will learn to adjust the signal timing dynamically to reduce congestion and improve vehicle movement efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7fd28",
   "metadata": {},
   "source": [
    "#### States\n",
    "The state represents the ***current condition*** of the ***traffic intersection***\n",
    "\n",
    "***NS*** : Sum of cars in the North-South direction.<br/>\n",
    "***EW***: Sum of cars in the East-West direction.<br/>\n",
    "***NS_array***: Array representing the number of cars in different positions in the North-South direction.<br/>\n",
    "***EW_array***: Array representing the number of cars in different positions in the East-West direction.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6516017",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "Actions represent the choices available to the agent that controls the traffic lights\n",
    "\n",
    "***Action 0*** ‚Äì Allow traffic to flow in the North-South (NS) direction (green light for NS, red light for EW). <br>\n",
    "***Action 1*** ‚Äì Allow traffic to flow in the East-West (EW) direction (green light for EW, red light for NS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee7e01",
   "metadata": {},
   "source": [
    "#### Discrete State Representation\n",
    "\n",
    "Given that the number of cars is kept between 0 and 5 for both directions (NS and EW), the state space is discrete with values ranging from 0 to 5 for both NS and EW.\n",
    "\n",
    "Discrete state can be a tuple (NS, EW) where both NS and EW are between 0 and 5:\n",
    "<br/>\n",
    "<br/>\n",
    "```state = (min(state[\"NS\"], 5), min(state[\"EW\"], 5))```\n",
    "\n",
    "#### State space\n",
    "For each intersection, there are 6 possible values (0 through 5) for both NS and EW, so the total number of possible discrete states is:\n",
    "\n",
    "State space: 6 (NS values) √ó 6 (EW values) = ***36 possible states***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393a9d1",
   "metadata": {},
   "source": [
    "#### State Action Diagram\n",
    "\n",
    "<img src=\"./StateActionDiagram.png\" alt=\"Alt text\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c9386",
   "metadata": {},
   "source": [
    "#### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd96a8d",
   "metadata": {},
   "source": [
    "- The reward function encourages clearing vehicles from the intersection, i.e., minimizing traffic.\n",
    "<br/>\n",
    "<br/>\n",
    "```reward = cleared```\n",
    "<br/>\n",
    "<br/>\n",
    "- Every time, agent take an action, vehicles in last selected directions are cleared, ***the number of vehicles cleared from intersection becomes rewards***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf59b73",
   "metadata": {},
   "source": [
    "#### State / Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a16055",
   "metadata": {},
   "source": [
    "```state = (min(NS_count, 5), min(EW_count, 5))```\n",
    "\n",
    "##### Value Function\n",
    "\n",
    "Estimated future reward from a state or state-action pair\n",
    "<br/>\n",
    "<br/>\n",
    "```q_table = {(ns, ew): [0, 0]}``` \n",
    "\n",
    "Updated using Q-Function\n",
    "<br/>\n",
    "<br/>\n",
    "```Q(s, a) ‚Üê Q(s, a) + Œ± * (r + Œ≥ * max(Q(s', a')) - Q(s, a))```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb544",
   "metadata": {},
   "source": [
    "#### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580a8b3",
   "metadata": {},
   "source": [
    "Following Greedy Policy\n",
    "\n",
    "```select_action(q_table, state, epsilon)```\n",
    "\n",
    "- With probability Œµ, choose a random action (exploration).\n",
    "- With probability 1‚àíŒµ, choose best known action (exploitation):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e76934",
   "metadata": {},
   "source": [
    "#### Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca3896",
   "metadata": {},
   "source": [
    "- Simulation is a classic finite MDP\n",
    "- Next state only depends on current state and action, not on past steps.\n",
    "- It includes randomness in vehicle inflow (modeled by np.random.choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09b221",
   "metadata": {},
   "source": [
    "#### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b0b36",
   "metadata": {},
   "source": [
    "- Although your current Q-learning algorithm uses table-based updates\n",
    "- Policy gradients or value gradients are used to update neural networks.<br/>\n",
    "<br/>\n",
    "```Q(s, a) ‚Üê Q(s, a) + Œ± * (target - Q(s, a))```\n",
    "<br/><br/>\n",
    "- Here, temporal difference (TD) update\n",
    "- a form of gradient descent over Q-values using:\n",
    "    - Œ±: learning rate (step size)\n",
    "    - target: reward + discounted future value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7ea9f",
   "metadata": {},
   "source": [
    "## Code Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9ad74",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab69e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "if sys.version_info < (3,12, 6):\n",
    "    sys.exit(\"This project requires Python 3.12.6.\")\n",
    "else:\n",
    "    print(\"Python version is compatible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'py3.12' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "## Install necessary libraries\n",
    "# To install the required libraries, run the following commands in a separate cell:\n",
    "!python -m pip install --upgrade pip\n",
    "!py -3.12 -m venv myenv venvRLTLOP\n",
    "print(\"Virtual environment created successfully\")\n",
    "!.\\venvRLTLOP\\Scripts\\Activate.ps1\n",
    "print(\"Virtual environment activated successfully\")\n",
    "!pip install -r requirements.txt\n",
    "print(\"All libraries installed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e8be3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8e3c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fadcfde",
   "metadata": {},
   "source": [
    "## Traffic Light Optimization using Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf6795",
   "metadata": {},
   "source": [
    "#### Class : TrafficIntersection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c477fa",
   "metadata": {},
   "source": [
    "```cars_ns``` : the number of cars at different positions for the north-south (NS) <br/>\n",
    "```cars_ew``` : the number of cars at different positions for the east-west (EW) <br/>\n",
    "```inflow_prob``` : The probability that a new car will enter the intersection from either direction <br/>\n",
    "```total_cleared``` : The total number of cars cleared from the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b306b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Traffic Environment ---------------- #\n",
    "class TrafficIntersection:\n",
    "\n",
    "    # Initializes the intersection with default values for the number of cars and inflow probability.\n",
    "    def __init__(self, inflow_prob=0.5):\n",
    "        self.cars_ns = np.zeros(5, dtype=int) \n",
    "        self.cars_ew = np.zeros(5, dtype=int)\n",
    "        self.inflow_prob = inflow_prob\n",
    "        self.total_cleared = 0\n",
    "        \n",
    "    # Resets the environment (cars at the intersection) and returns the current state.\n",
    "    def reset(self):\n",
    "        self.cars_ns[:] = 0\n",
    "        self.cars_ew[:] = 0\n",
    "        self.total_cleared = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    # Executes a step in the environment. \n",
    "    # It takes an action (0 for NS green light, 1 for EW green light), \n",
    "    # updates the state, car positions, and clears cars based on the action.\n",
    "    def step(self, action):\n",
    "        cleared = 0\n",
    "        if action == 0:\n",
    "            cleared = self.cars_ns[-1]\n",
    "            self.cars_ns[1:] = self.cars_ns[:-1]\n",
    "            self.cars_ns[0] = np.random.choice([0, 1], p=[1 - self.inflow_prob, self.inflow_prob])\n",
    "            self.cars_ew += np.random.choice([0, 1], size=5, p=[0.7, 0.3])\n",
    "        else:\n",
    "            cleared = self.cars_ew[-1]\n",
    "            self.cars_ew[1:] = self.cars_ew[:-1]\n",
    "            self.cars_ew[0] = np.random.choice([0, 1], p=[1 - self.inflow_prob, self.inflow_prob])\n",
    "            self.cars_ns += np.random.choice([0, 1], size=5, p=[0.7, 0.3])\n",
    "\n",
    "        self.cars_ns = np.clip(self.cars_ns, 0, 1)\n",
    "        self.cars_ew = np.clip(self.cars_ew, 0, 1)\n",
    "\n",
    "        self.total_cleared += cleared\n",
    "        reward = cleared\n",
    "        state = self.get_state()\n",
    "        return state, reward, reward > 0\n",
    "\n",
    "    # Returns the current state of the intersection\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            \"NS\": self.cars_ns.sum(),\n",
    "            \"EW\": self.cars_ew.sum(),\n",
    "            \"NS_array\": self.cars_ns.copy(),\n",
    "            \"EW_array\": self.cars_ew.copy()\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330063e",
   "metadata": {},
   "source": [
    "#### Q-Learning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a4182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the state (number of cars in NS and EW directions) into a discrete representation suitable for Q-learning.\n",
    "def get_discrete_state(state):\n",
    "    return (min(state[\"NS\"], 5), min(state[\"EW\"], 5))\n",
    "\n",
    "# Chooses an action (0 or 1) based on the epsilon-greedy strategy. \n",
    "# It either selects a random action or the best action based on the Q-table.\n",
    "def select_action(q_table, state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice([0, 1])\n",
    "    return np.argmax(q_table[state])\n",
    "\n",
    "# Updates the Q-table using the Q-learning update rule, based on the current state, action taken, reward, and the next state.\n",
    "def update_q_table(q_table, state, action, reward, next_state, alpha, gamma):\n",
    "    old_value = q_table[state][action]\n",
    "    future_max = np.max(q_table[next_state])\n",
    "    new_value = old_value + alpha * (reward + gamma * future_max - old_value)\n",
    "    q_table[state][action] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046add3",
   "metadata": {},
   "source": [
    "#### Agent Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b95ab3",
   "metadata": {},
   "source": [
    "- Trains the agents using ***Q-learning*** for ```n_intersections``` intersections over ```episodes``` number of episodes.\n",
    "- For each intersection, a Q-table is initialized, and the traffic light action is taken according to the Q-learning policy.\n",
    "- The environment is updated, and the Q-table is updated based on the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df2091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agents(n_intersections, episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    q_tables = []\n",
    "    all_rewards = []\n",
    "    inflow_probs = np.linspace(0.3, 0.7, n_intersections)\n",
    "\n",
    "    for i in range(n_intersections):\n",
    "        q_table = {(ns, ew): [0, 0] for ns in range(6) for ew in range(6)}\n",
    "        rewards_per_episode = []\n",
    "        env = TrafficIntersection(inflow_prob=inflow_probs[i])\n",
    "\n",
    "        for ep in range(episodes):\n",
    "            state = get_discrete_state(env.reset())\n",
    "            total_reward = 0\n",
    "            for step in range(50):\n",
    "                action = select_action(q_table, state, epsilon)\n",
    "                next_state, reward, _ = env.step(action)\n",
    "                next_state = get_discrete_state(next_state)\n",
    "                update_q_table(q_table, state, action, reward, next_state, alpha, gamma)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            rewards_per_episode.append(total_reward)\n",
    "\n",
    "        q_tables.append(q_table)\n",
    "        all_rewards.append(rewards_per_episode)\n",
    "\n",
    "    return q_tables, all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14a927",
   "metadata": {},
   "source": [
    "#### Demo Agent logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cc2926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a visualization of the traffic intersection using matplotlib\n",
    "def draw_intersection(state, action, step, idx, countdown, highlight_clear):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_facecolor(\"#2e2e2e\")\n",
    "    ax.add_patch(plt.Rectangle((0, 4.5), 10, 1, color=\"#444\"))\n",
    "    ax.add_patch(plt.Rectangle((4.5, 0), 1, 10, color=\"#444\"))\n",
    "    for i in range(0, 10, 1):\n",
    "        ax.plot([i, i + 0.5], [5.0, 5.0], color=\"white\", linewidth=1, linestyle=\"--\")\n",
    "        ax.plot([5.0, 5.0], [i, i + 0.5], color=\"white\", linewidth=1, linestyle=\"--\")\n",
    "    light_ns_color = \"green\" if action == 0 else \"red\"\n",
    "    light_ew_color = \"green\" if action == 1 else \"red\"\n",
    "    ax.add_patch(plt.Circle((5, 9), 0.4, color=light_ns_color))\n",
    "    ax.text(5, 8.3, f\"{countdown}s\", ha='center', va='center', fontsize=10, color='black', bbox=dict(facecolor='white', boxstyle='round,pad=0.2'))\n",
    "    ax.add_patch(plt.Circle((9, 5), 0.4, color=light_ew_color))\n",
    "    ax.text(8.2, 5, f\"{countdown}s\", ha='center', va='center', fontsize=10, color='black', bbox=dict(facecolor='white', boxstyle='round,pad=0.2'))\n",
    "\n",
    "    for i in range(5):\n",
    "        if state[\"NS_array\"][i]:\n",
    "            ax.add_patch(plt.Rectangle((4.6, 9 - i), 0.8, 0.5, color=\"red\", alpha=0.9))\n",
    "        if state[\"EW_array\"][i]:\n",
    "            ax.add_patch(plt.Rectangle((i, 4.6), 0.5, 0.8, color=\"blue\", alpha=0.9))\n",
    "\n",
    "    if highlight_clear:\n",
    "        ax.text(5, 5.2, \"+1\", ha='center', va='center', fontsize=14, color='lime', fontweight='bold')\n",
    "    ax.text(5, 1, \"S ‚Üì\", ha='center', va='center', fontsize=9, color='white')\n",
    "    ax.text(5, 9.3, \"N ‚Üë\", ha='center', va='center', fontsize=9, color='white')\n",
    "    ax.text(1.2, 5, \"W ‚Üê\", ha='center', va='center', fontsize=9, color='white')\n",
    "    ax.text(8.8, 5, \"E ‚Üí\", ha='center', va='center', fontsize=9, color='white')\n",
    "    fig.text(0.5, 1.0, f\"Intersection {idx+1}\", ha='center', fontsize=12, fontweight='bold', color='black')\n",
    "    fig.text(0.5, -0.05, \"üü• Red = Stop     üü© Green = Go     ‚è± = Countdown     +1 = Vehicle Passed\", ha='center', fontsize=9, color='black', bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.4'))\n",
    "    return fig\n",
    "\n",
    "# This function simulates the traffic flow over a series of steps for all intersections,\n",
    "# showing a real-time update of the environment using the learned Q-table.\n",
    "def demo_agents(q_tables, steps=30, speed=0.4):\n",
    "    inflow_probs = np.linspace(0.3, 0.7, len(q_tables))\n",
    "    envs = [TrafficIntersection(inflow_prob=prob) for prob in inflow_probs]\n",
    "    states = [get_discrete_state(env.reset()) for env in envs]\n",
    "    placeholders = [st.empty() for _ in q_tables]\n",
    "    intersection_states = [{\"action\": 0, \"timer\": 3} for _ in q_tables]\n",
    "\n",
    "    for t in range(steps):\n",
    "        for i, (env, q_table) in enumerate(zip(envs, q_tables)):\n",
    "            if intersection_states[i][\"timer\"] == 0:\n",
    "                intersection_states[i][\"action\"] = 1 - intersection_states[i][\"action\"]\n",
    "                intersection_states[i][\"timer\"] = 3\n",
    "            action = intersection_states[i][\"action\"]\n",
    "            intersection_states[i][\"timer\"] -= 1\n",
    "            full_state, _, cleared = env.step(action)\n",
    "            fig = draw_intersection(full_state, action, t, i, intersection_states[i][\"timer\"] + 1, highlight_clear=cleared)\n",
    "            placeholders[i].pyplot(fig)\n",
    "            plt.close(fig)\n",
    "            states[i] = get_discrete_state(full_state)\n",
    "        time.sleep(speed)\n",
    "\n",
    "    st.subheader(\"üöó Total Vehicles Cleared per Intersection\")\n",
    "    for i, env in enumerate(envs):\n",
    "        st.markdown(f\"**Intersection {i+1}:** {env.total_cleared} vehicles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d46f524",
   "metadata": {},
   "source": [
    "#### Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b100a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 18:27:25.001 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.920 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Khushbu.Lad\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-13 18:27:25.922 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.925 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.926 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.938 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.939 Session state does not function when running a script without `streamlit run`\n",
      "2025-04-13 18:27:25.942 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.943 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.955 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.956 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.959 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.960 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.969 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.975 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.976 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:25.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:42.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:42.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:42.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:42.695 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-04-13 18:27:45.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Streamlit UI ---------------- #\n",
    "st.title(\"Multi-Intersection Traffic Light Optimization (Q-learning)\")\n",
    "\n",
    "mode = st.selectbox(\"Choose Mode\", [\"Train Agent\", \"Demo Agent\"])\n",
    "n_intersections = st.slider(\"Number of Intersections\", 1, 4, 2)\n",
    "speed = st.slider(\"Demo Speed (sec/frame)\", 0.1, 1.0, 0.4)\n",
    "\n",
    "q_file = f\"q_tables_{n_intersections}.pkl\"\n",
    "\n",
    "if mode == \"Train Agent\":\n",
    "    st.info(\"Training Q-learning agents...\")\n",
    "    q_tables, rewards_list = train_agents(n_intersections)\n",
    "    st.success(\"Training complete!\")\n",
    "    with open(q_file, \"wb\") as f:\n",
    "        pickle.dump(q_tables, f)\n",
    "    for i, rewards in enumerate(rewards_list):\n",
    "        st.subheader(f\"Intersection {i+1}\")\n",
    "        st.line_chart(rewards)\n",
    "\n",
    "elif mode == \"Demo Agent\":\n",
    "    if os.path.exists(q_file):\n",
    "        with open(q_file, \"rb\") as f:\n",
    "            q_tables = pickle.load(f)\n",
    "        st.success(\"Loaded trained Q-tables.\")\n",
    "        demo_agents(q_tables, steps=30, speed=speed)\n",
    "    else:\n",
    "        st.error(\"No Q-tables found. Please train the agent(s) first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c50203",
   "metadata": {},
   "source": [
    "#### Simulation : Run command in terminal\n",
    "\n",
    "```streamlit run \"multi_intersection_sim.py\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a081480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run multi_intersection_sim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7803f46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
